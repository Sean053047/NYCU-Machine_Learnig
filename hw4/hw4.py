import numpy as np 
import matplotlib.pyplot as plt
from tqdm import tqdm
from numba import jit
MAX_ITERATION_REGRESSION = 5000
MAX_ITERATION_EM = 50
np.random.seed(0)
# Utils
np.set_printoptions(threshold=np.inf)
        
def gaussian_generate(mu, var, N):
    '''
    Step 1. Box-Muller transform
    Step 2. Affine transform:
    Any Gaussian distribution can be generated by isotopic Gaussian via affine transform.
    '''
    u1 = np.random.uniform(0, 1, N)
    u2 = np.random.uniform(0, 1, N)
    x = np.sqrt(-2 * np.log(u1)) * np.cos(2*np.pi * u2)                        
    return x*np.sqrt(var) + mu    

def read_idx3_ubyte(fpth:str, threshold=None):
    with open(fpth, 'rb') as file:
        byte_data = file.read()
    rows = int.from_bytes(byte_data[8:12], 'big')
    cols = int.from_bytes(byte_data[12:16], 'big')
    # Set an image into a vector.
    data = np.frombuffer(byte_data[16:], dtype=np.uint8).reshape((-1, rows*cols),order='C')
    if threshold is not None:
        data = (data > threshold).astype(np.int32)
    return rows, cols, data

def read_idx1_ubyte(fpth:str):
    with open(fpth, 'rb') as file:
        byte_data = file.read()
    label = np.frombuffer(byte_data[8:], dtype=np.uint8)
    label_type = np.unique(label)
    return label, label_type

# Part1: Logistic regression
class LR:
    @staticmethod
    def get_design_matrix(arr):
        N = arr.shape[0]
        return np.concatenate((np.ones((N, 1)), arr), axis=1)
    
    @staticmethod
    def logistic(arr):
        return 1 / (1 + np.exp(- arr))
    
    @classmethod
    def predict(cls, D, W, threshold=0.5):
        phi = cls.get_design_matrix(D)
        logistic_value = cls.logistic(phi@W)
        return  (threshold < logistic_value).flatten().astype(np.int32)
    
    @classmethod
    def get_confusion_matrix(cls, D, W,  Label, threshold=0.5):
        '''
        phi  : N* k matrix,
        W    : K* 1 matrix
        Label: N* 1 matrix'''
        pred = cls.predict(D, W, threshold)
        Label = Label.flatten()
        #record[ (pred, gt) ]
        record = {
            (i,j): int(np.sum((pred == i) & (Label == j)))
            for i in (0, 1) for j in (0, 1)
        }
        
        return record
    @classmethod
    def print_result(cls, title:str, W, record):
        print("="*20)
        print(f"{title}:")
        print(f"W:")
        for w in W:
            print(f"{float(w[0])}")
        print("\nConfusion Matrix:")
        print("\t\tPredict cluster 1\tPredict cluster 2")
        print(f"Is cluster 1\t\t{record[(0, 0)]}\t\t\t{record[(0, 1)]}")
        print(f"Is cluster 2\t\t{record[(1, 0)]}\t\t\t{record[(1, 1)]}")
        print("")
        print(f"Specificity: (Successfully predict cluster 1 (0)): ", record[(0,0)] / (record[(0,0)] + record[(1,0)]))
        print(f"Sensitivity: (Successfully predict cluster 2 (1)): ", record[(1,1)] / (record[(0,1)] + record[(1,1)]))
        print("="*20)

def logistic_regression(N, means, variances):
    # Generate data
    mx1, my1, mx2, my2 = means
    vx1, vy1, vx2, vy2 = variances
    D1 = np.array( [gaussian_generate(mx1, vx1, N), gaussian_generate(my1, vy1, N)]).T
    D2 = np.array( [gaussian_generate(mx2, vx2, N), gaussian_generate(my2, vy2, N)]).T
    
    D = np.concatenate((D1, D2), axis=0)
    Label =  np.concatenate((np.zeros(D1.shape[0]), np.ones(D2.shape[0]))).reshape((-1, 1)) # Assign D1=>0 ,  D2=>1
    # Assumption: 
    #       [[w0], 
    #  W =  [w1],    3*1 matrix
    #       [w2]]
    # Wt * Phi = W0 + W1*x + W2*y
    Wg = np.zeros((3, 1)) # ? Initial Guess
    phi = LR.get_design_matrix(D)
    # Gradient descent
    lrs = {0: 1e-3, 100:1e-1, 500:5e-4}    
    for i in range(MAX_ITERATION_REGRESSION):
        lr = lrs[i] if i in lrs else lr
        gradient = phi.T @ (LR.logistic(phi@Wg) - Label)
        Wg = Wg - lr * gradient
        if np.linalg.norm(gradient) < 1e-2:
            print(f"Jump out loop-gradient descent.\nIterate {i+1} times")
            break
    else:
        print(f"Jump out loop-gradient descent.\nIterate {i+1} times")
    LR.print_result("Gradient Descent", Wg, record=LR.get_confusion_matrix(D, Wg, Label))
    
    # Newton's Method
    Wn = np.zeros((3,1))
    prior_Wn = np.zeros_like(Wn)
    for i in range(MAX_ITERATION_REGRESSION):
        fx = LR.logistic(phi@Wn)
        gradient = phi.T @ (fx - Label)
        
        D_matrix = np.diag((fx*(1-fx)).flatten())
        hessian = phi.T @ D_matrix @ phi
        if np.linalg.det(hessian) ==0:
            # print(f"Determinant: {hessian}, determinant: {np.linalg.det(hessian)}")
            hessian = np.eye(gradient.shape[0]) * 1e-4
        Wn = Wn - np.linalg.inv(hessian) @ gradient
        if np.linalg.norm(Wn - prior_Wn) < 1e-4:
            print(f"Jump out loop-Newton's method.\nIterate {i+1} times.")
            break
        prior_Wn = Wn
    else:
        print(f"Jump out loop-Newton's method.\nIterate {i+1} times.")
    LR.print_result("Newton's Method", Wn, record=LR.get_confusion_matrix(D, Wn, Label))
    
    plot_LR_result(D1, D2, Wg, Wn)
    
def plot_LR_result(D1, D2, Wg, Wn, threshold=0.5):
    '''
    D: N*2
    '''
    _, axes = plt.subplots(1,3)
    axes = axes.flatten()
    # Plot Ground Truth
    axes[0].scatter(D1[:,0], D1[:, 1], label='D1',color='r')
    axes[0].scatter(D2[:,0], D2[:, 1], label='D2',color='b')
    axes[0].set_title("Ground Truth")
    axes[0].legend()
    
    D = np.concatenate((D1, D2), axis=0)
    pred_g = LR.predict(D, Wg, threshold)
    axes[1].scatter(D[pred_g==0, 0], D[pred_g==0, 1], label='D1', color='r')
    axes[1].scatter(D[pred_g==1, 0], D[pred_g==1, 1], label='D2', color='b')
    axes[1].set_title("Steepest Descent")
    axes[1].legend()
    
    pred_n = LR.predict(D, Wn, threshold)
    axes[2].scatter(D[pred_n==0, 0], D[pred_n==0, 1], label='D1', color='r')
    axes[2].scatter(D[pred_n==1, 0], D[pred_n==1, 1], label='D2', color='b')
    axes[2].set_title("Newton's Method")
    axes[2].legend()
    plt.show()

# Part2: MNIST classifier via EM algorithm
class EMModel:
    def __init__(self, rows, cols, label_type, batch_size=30):
        self.im_shape = (rows, cols)
        self.label_type = label_type
        
        self.prob_head = np.random.uniform(0, 1, (self.num_cls, self.size)) # (num_class, size)
        # self.prob_head = np.ones((self.num_cls, self.size))*0.5
        self.prob_choose = np.full((self.num_cls, self.size), 1/self.num_cls, dtype=np.float64)
        
        self.batch_size = batch_size
    def E_step(self, data):
        '''responsibility: (num_data * num_class * size) array
        here we want to get the responsibility of each pixel at each given image.
        '''
        num_data = data.shape[0]
        tmp_data = data.copy().reshape(num_data, 1, self.size)  # (num data, 1, size)
        tmp_prob_head = self.prob_head.reshape(1, self.num_cls, self.size)  # (1, num class, size)
        tmp_prob_choose = self.prob_choose.reshape(1, self.num_cls, self.size) # (1, num class, size)
        responsibility = tmp_prob_choose*(tmp_data*tmp_prob_head + (1- tmp_data) * (1-tmp_prob_head)) # (num_data, num_class, size)
        # Normalize responsibility
        sum_across_class = np.zeros((num_data, 1, self.size)) # (num_data, 1, size)
        for cc in range(self.num_cls):    
            sum_across_class += responsibility[:, cc:cc+1, :]
        sum_across_class[sum_across_class< 1e-4] = np.inf
        
        responsibility =  responsibility/sum_across_class
        return responsibility
        
    def M_step(self, data, responsibility):
        def __sum_across_data(arr):
            summation = np.zeros(arr.shape[1:])
            for ii in range(arr.shape[0]):
                summation += arr[ii, :, :]
            return summation
        num_data = data.shape[0]
        # Update prob_choose
        res_sum = __sum_across_data(responsibility)
        res_sum[res_sum < 1e-4] = np.inf
        new_prob_choose = res_sum / num_data   # * Something I can't derive the correct answer bute it works.
        # Update prob head
        nominator = __sum_across_data( responsibility * data.copy().reshape(num_data, 1, self.size))
        new_prob_head = nominator / res_sum
        
        return new_prob_choose, new_prob_head
    @property
    def num_cls(self):
        return len(self.label_type)
    @property
    def size(self):
        return self.im_shape[0] * self.im_shape[1]
    
    def load_batch(self, data):
        num_data = data.shape[0]
        tmp_data = np.copy(data)
        np.random.shuffle(tmp_data) # Shuffle along first axis
        for indx in range( self.batch_size, num_data+1, self.batch_size):
            yield tmp_data[indx-self.batch_size:indx]
            
    def fit(self, train_data, SAVE_WEIGHT=True, wc_pth="", wh_pth=""):
        for i in tqdm(range(MAX_ITERATION_EM), desc="EM Training: "):
            old_prob_choose, old_prob_head = self.prob_choose, self.prob_head
        
            for batch_data in self.load_batch(train_data):
                responsibility = self.E_step(batch_data)
                self.prob_choose, self.prob_head = self.M_step(batch_data, responsibility)
            # plot_likelihood_images(self.get_likelihood_image())
                
            if self.converge_criterion(old_prob_choose, old_prob_head):
                print("After {i} iterations, training converge!")
                break
        else:
            print(f"End of {MAX_ITERATION_EM} iterations.")
        if SAVE_WEIGHT:
            np.save(wc_pth, self.prob_choose)
            np.save(wh_pth, self.prob_head)
            print("Save weight.")
        
    def converge_criterion(self, old_prob_choose, old_prob_head):
        condition1 = np.abs( self.prob_choose - old_prob_choose) < 1e-3
        condition2 = np.abs( self.prob_head - old_prob_head) < 1e-2
        ratio1 = np.sum(condition1)/ (self.num_cls*self.size)
        ratio2 = np.sum(condition2)/ (self.num_cls*self.size)
        print(f"Condition 1: {ratio1*100:.2f}% | Condition 2: {ratio2*100:.2f}%")
        
        return True if ratio1> 0.8 and ratio2 > 0.7 else False
        
    def load_pretrained(self, wc_pth:str, wh_pth:str) -> None:
        self.prob_choose = np.load(wc_pth)
        self.prob_head = np.load(wh_pth)
    
    def assign_class(self, train_data, train_label):
        '''Use voting system to determine which class belongs to which number.'''
        preds = self.predict(train_data)
        
        
    def predict(self, pred_data):
        '''For each data'''
        num_data = pred_data.shape[0]
        tmp_data =pred_data.reshape(num_data, 1 , self.size)
        tmp_prob_head = self.prob_head.copy().reshape(1, self.num_cls, self.size)
        tmp_prob_choose = self.prob_choose.copy().reshape(1, self.num_cls, self.size)
        log_likelihoods = np.log(tmp_prob_choose*(tmp_data*tmp_prob_head + (1-tmp_data) * (1-tmp_prob_head)))
        
        data_log_likelihoods = np.zeros((num_data, self.num_cls), dtype=np.float64)
        for sz in range(self.size):
            data_log_likelihoods += log_likelihoods[:, :, sz]
        results = np.argmax(data_log_likelihoods, axis=1)
        return results.flatten()
        
    def confusion_matrix(self, pred, label):
        ...
    def get_likelihood_image(self, threshold =0.5):
        self.matching = {i:i for i in range(self.num_cls)}
        images = {
            self.matching[i]: (image>threshold).reshape(self.im_shape)
                for i, (image) in enumerate(self.prob_head)
        }
        return images
    
def em_algorithm(rows, cols, train_data, train_label, test_data, test_label, label_type):
    global args
    em_model = EMModel(rows, cols, label_type, batch_size=args.batch_size)
    plot_likelihood_images(em_model.get_likelihood_image())
    
    if args.em_mode == "train":
        em_model.fit(train_data, 
                     SAVE_WEIGHT=args.save_weight, 
                     wc_pth=args.prob_choose_pth, 
                     wh_pth=args.prob_head_pth)
    elif args.em_mode == "load":
        em_model.load_pretrained(args.prob_choose_pth, args.prob_head_pth)
    
    # em_model.assign_class(train_data, train_label)
    # em_model.predict(train_data)
    plot_likelihood_images(em_model.get_likelihood_image())
    
    
def plot_likelihood_images(images:dict):
    fig = plt.figure("likelihood images", figsize=(10,5))
    axes = fig.subplots(2,5 )
    axes = axes.flatten()
    for i, ax in enumerate(axes):
        img = images[i].astype(np.uint8) *255
        
        ax.imshow(img, cmap='gray')
        ax.set_title(f"digit: {i}")
        ax.axis('off') 
    fig.tight_layout()
    plt.show()
    
    # plt.pause(1)
    
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(prog="Machine Learning Homework 4.")
    parser.add_argument("mode", type=str, help="LR | EM")
    
    # EM setting
    parser.add_argument("--em_mode", type=str, default="train", help="train | load")
    parser.add_argument("--batch_size", type=int, default=30)
    parser.add_argument("--save_weight", action="store_true")
    parser.add_argument("--prob_choose_pth", type=str, default="./prob_choose.npy")
    parser.add_argument("--prob_head_pth", type=str, default="./prob_head.npy")
    args = parser.parse_args()
    
    if args.mode == "LR":
        N = int(input("N: "))
        means = [float(ii) for ii in str(input("mx1, my1, mx2, my2: ")).split(" ")]
        variances = [float(ii) for ii in str(input("vx1, vy1, vx2, vy2: ")).split(" ")]
        logistic_regression(N, means, variances)
    elif args.mode == "EM":
        rows, cols, train_data = read_idx3_ubyte("./data/train-images.idx3-ubyte_", threshold=127)
        train_label, label_type = read_idx1_ubyte("./data/train-labels.idx1-ubyte_")
        _, _, test_data = read_idx3_ubyte("./data/t10k-images.idx3-ubyte_", threshold=127)
        test_label, label_type = read_idx1_ubyte("./data/t10k-labels.idx1-ubyte_")
        
        em_algorithm(rows, cols, train_data, train_label, test_data, test_label, label_type)
    else:
        assert False, "Wrong input mode."