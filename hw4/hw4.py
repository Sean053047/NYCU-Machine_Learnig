import numpy as np 
import matplotlib.pyplot as plt
from numba import jit
MAX_ITERATION = 5000
np.random.seed(13)
# Utils
np.set_printoptions(threshold=np.inf)
        
def gaussian_generate(mu, var, N):
    '''
    Step 1. Box-Muller transform
    Step 2. Affine transform:
    Any Gaussian distribution can be generated by isotopic Gaussian via affine transform.
    '''
    u1 = np.random.uniform(0, 1, N)
    u2 = np.random.uniform(0, 1, N)
    x = np.sqrt(-2 * np.log(u1)) * np.cos(2*np.pi * u2)                        
    return x*np.sqrt(var) + mu    

def read_idx3_ubyte(fpth:str, threshold=None):
    with open(fpth, 'rb') as file:
        byte_data = file.read()
    rows = int.from_bytes(byte_data[8:12], 'big')
    cols = int.from_bytes(byte_data[12:16], 'big')
    # Set an image into a vector.
    data = np.frombuffer(byte_data[16:], dtype=np.uint8).reshape((-1, rows*cols),order='C')
    if threshold is not None:
        data = (data > threshold).astype(np.int32)
    return rows, cols, data

def read_idx1_ubyte(fpth:str):
    with open(fpth, 'rb') as file:
        byte_data = file.read()
    label = np.frombuffer(byte_data[8:], dtype=np.uint8)
    label_type = np.unique(label)
    return label, label_type

# Part1: Logistic regression

class LR:
    @staticmethod
    def get_design_matrix(arr):
        N = arr.shape[0]
        return np.concatenate((np.ones((N, 1)), arr), axis=1)
    
    @staticmethod
    def logistic(arr):
        return 1 / (1 + np.exp(- arr))
    
    @classmethod
    def predict(cls, D, W, threshold=0.5):
        phi = cls.get_design_matrix(D)
        logistic_value = cls.logistic(phi@W)
        return  (threshold < logistic_value).flatten().astype(np.int32)
    
    @classmethod
    def get_confusion_matrix(cls, D, W,  Label, threshold=0.5):
        '''
        phi  : N* k matrix,
        W    : K* 1 matrix
        Label: N* 1 matrix'''
        pred = cls.predict(D, W, threshold)
        Label = Label.flatten()
        #record[ (pred, gt) ]
        record = {
            (i,j): int(np.sum((pred == i) & (Label == j)))
            for i in (0, 1) for j in (0, 1)
        }
        
        return record
    @classmethod
    def print_result(cls, title:str, W, record):
        print("="*20)
        print(f"{title}:")
        print(f"W:")
        for w in W:
            print(f"{float(w[0])}")
        print("\nConfusion Matrix:")
        print("\t\tPredict cluster 1\tPredict cluster 2")
        print(f"Is cluster 1\t\t{record[(0, 0)]}\t\t\t{record[(0, 1)]}")
        print(f"Is cluster 2\t\t{record[(1, 0)]}\t\t\t{record[(1, 1)]}")
        print("")
        print(f"Specificity: (Successfully predict cluster 1 (0)): ", record[(0,0)] / (record[(0,0)] + record[(1,0)]))
        print(f"Sensitivity: (Successfully predict cluster 2 (1)): ", record[(1,1)] / (record[(0,1)] + record[(1,1)]))
        print("="*20)

def logistic_regression(N, means, variances):
    # Generate data
    mx1, my1, mx2, my2 = means
    vx1, vy1, vx2, vy2 = variances
    D1 = np.array( [gaussian_generate(mx1, vx1, N), gaussian_generate(my1, vy1, N)]).T
    D2 = np.array( [gaussian_generate(mx2, vx2, N), gaussian_generate(my2, vy2, N)]).T
    
    D = np.concatenate((D1, D2), axis=0)
    Label =  np.concatenate((np.zeros(D1.shape[0]), np.ones(D2.shape[0]))).reshape((-1, 1)) # Assign D1=>0 ,  D2=>1
    # Assumption: 
    #       [[w0], 
    #  W =  [w1],    3*1 matrix
    #       [w2]]
    # Wt * Phi = W0 + W1*x + W2*y
    Wg = np.zeros((3, 1)) # ? Initial Guess
    phi = LR.get_design_matrix(D)
    # Gradient descent
    lrs = {0: 1e-3, 100:1e-1, 500:5e-4}    
    for i in range(MAX_ITERATION):
        lr = lrs[i] if i in lrs else lr
        gradient = phi.T @ (LR.logistic(phi@Wg) - Label)
        Wg = Wg - lr * gradient
        if np.linalg.norm(gradient) < 1e-2:
            print(f"Jump out loop-gradient descent.\nIterate {i+1} times")
            break
    else:
        print(f"Jump out loop-gradient descent.\nIterate {i+1} times")
    LR.print_result("Gradient Descent", Wg, record=LR.get_confusion_matrix(D, Wg, Label))
    
    # Newton's Method
    Wn = np.zeros((3,1))
    prior_Wn = np.zeros_like(Wn)
    for i in range(MAX_ITERATION):
        fx = LR.logistic(phi@Wn)
        gradient = phi.T @ (fx - Label)
        
        D_matrix = np.diag((fx*(1-fx)).flatten())
        hessian = phi.T @ D_matrix @ phi
        if np.linalg.det(hessian) ==0:
            # print(f"Determinant: {hessian}, determinant: {np.linalg.det(hessian)}")
            hessian = np.eye(gradient.shape[0]) * 1e-4
        Wn = Wn - np.linalg.inv(hessian) @ gradient
        if np.linalg.norm(Wn - prior_Wn) < 1e-4:
            print(f"Jump out loop-Newton's method.\nIterate {i+1} times.")
            break
        prior_Wn = Wn
    else:
        print(f"Jump out loop-Newton's method.\nIterate {i+1} times.")
    LR.print_result("Newton's Method", Wn, record=LR.get_confusion_matrix(D, Wn, Label))
    
    plot_LR_result(D1, D2, Wg, Wn)
    
def plot_LR_result(D1, D2, Wg, Wn, threshold=0.5):
    '''
    D: N*2
    '''
    _, axes = plt.subplots(1,3)
    axes = axes.flatten()
    # Plot Ground Truth
    axes[0].scatter(D1[:,0], D1[:, 1], label='D1',color='r')
    axes[0].scatter(D2[:,0], D2[:, 1], label='D2',color='b')
    axes[0].set_title("Ground Truth")
    axes[0].legend()
    
    D = np.concatenate((D1, D2), axis=0)
    pred_g = LR.predict(D, Wg, threshold)
    axes[1].scatter(D[pred_g==0, 0], D[pred_g==0, 1], label='D1', color='r')
    axes[1].scatter(D[pred_g==1, 0], D[pred_g==1, 1], label='D2', color='b')
    axes[1].set_title("Steepest Descent")
    axes[1].legend()
    
    pred_n = LR.predict(D, Wn, threshold)
    axes[2].scatter(D[pred_n==0, 0], D[pred_n==0, 1], label='D1', color='r')
    axes[2].scatter(D[pred_n==1, 0], D[pred_n==1, 1], label='D2', color='b')
    axes[2].set_title("Newton's Method")
    axes[2].legend()
    plt.show()


# Part2: MNIST classifier via EM algorithm

class EMModel:
    def __init__(self, rows, cols, label_type):
        self.im_shape = (rows, cols)
        self.label_type = label_type
        
        self.prob_head = np.random.uniform(0, 1, (self.num_cls, rows*cols))
        self.prob_choose = np.ones((self.num_cls, rows*cols)) / len(label_type) 
        
    def E_step(self, data):
        '''responsibility: (num_data * num_class * size) array
        here we want to get the responsibility of each pixel at each given image.
        '''
        num_data = data.shape[0]
        tmp_data = data.reshape(num_data, 1, self.size)  # (num data, 1, size)
        tmp_prob_head = self.prob_head.reshape(1, self.num_cls, self.size)  # (1, num class, size)
        tmp_prob_choose = self.prob_choose.reshape(1, self.num_cls, self.size) # (1, num class, size)
        responsibility = tmp_prob_choose*(tmp_data*tmp_prob_head + (1- tmp_data) * (1-tmp_prob_head)) # (num_data, num_class, size)
        # Normalize responsibility
        sum_across_class = np.zeros((num_data, 1, self.size)) # (num_data, 1, size)
        for cc in range(self.num_cls):    
            sum_across_class += responsibility[:, [cc], :]
            
        responsibility =  responsibility/sum_across_class
        return responsibility
        
    def M_step(self, data, responsibility):...

    @property
    def num_cls(self):
        return len(self.label_type)
    @property
    def size(self):
        return self.im_shape[0] * self.im_shape[1]
    
    def fit(self, data):
        for i in range(MAX_ITERATION):
            responsibility = self.E_step(data)
            self.M_step(data, responsibility)
            
    def predict(self, data):
        ...
    def confusion_matrix(self, pred, label):
        ...
    def get_likelihood_image(self):
        ...
    
def em_algorithm(rows, cols, train_data, test_data, test_label, label_type):
    num_cls = len(label_type)
    em_model = EMModel(rows, cols, label_type)
    em_model.fit(train_data)
    
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(prog="Machine Learning Homework 4.")
    parser.add_argument("mode", type=str, help="LR | EM")
    args = parser.parse_args()
    if args.mode == "mode":
        N = int(input("N: "))
        means = [float(ii) for ii in str(input("mx1, my1, mx2, my2: ")).split(" ")]
        variances = [float(ii) for ii in str(input("vx1, vy1, vx2, vy2: ")).split(" ")]
        logistic_regression(N, means, variances)
    elif args.mode == "EM":
        rows, cols, train_data = read_idx3_ubyte("./data/train-images.idx3-ubyte_", threshold=127)
        _, _, test_data = read_idx3_ubyte("./data/t10k-images.idx3-ubyte_", threshold=127)
        test_label, label_type = read_idx1_ubyte("./data/t10k-labels.idx1-ubyte_")
        
        em_algorithm(rows, cols, train_data, test_data, test_label, label_type)
    else:
        raise "Wrong input mode."