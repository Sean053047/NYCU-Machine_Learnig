import numpy as np 
import argparse
import matplotlib.pyplot as plt 
ONLINE_LEARNING_LIMIT = 800

def gaussian_generate(mu, var, N):
    '''
    Step 1. Box-Muller transform
    Step 2. Affine transform:
    Any Gaussian distribution can be generated by isotopic Gaussian via affine transform.
    '''
    u1 = np.random.uniform(0, 1, N)
    u2 = np.random.uniform(0, 1, N)
    x = np.sqrt(-2 * np.log(u1)) * np.cos(2*np.pi * u2)                        
    return x*np.sqrt(var) + mu    

def polynomial_basis(x, n, order='nN'):
    ''' Return n x N array'''
    data = np.array([x**i for i in range(n)])       
    return data if order == 'nN' else data.T if order =='Nn' else None

def poly_linear_model_data_generator(num_basis, W, noise_var, N, render=False):
    assert num_basis == len(W), "Weights don't have enough elements."
    x = np.random.uniform(-1.0, 1.0, N)
    e = gaussian_generate(0, noise_var, N)
    phi = polynomial_basis(x , num_basis, order="Nn")
    W = np.array(W).reshape((-1, 1))
    y = (phi @ W).flatten() + e 
    if render:
        _, ax = plt.subplots()
        gt_x = np.arange(-1.1, 1.1, 0.1)
        gt_y = polynomial_basis(gt_x, n, "Nn") @ W
        ax.scatter(x.flatten(), y.flatten(), label="Generated data")
        ax.plot(gt_x.flatten(), gt_y.flatten(), label="Ground Truth", color='r')
        ax.set_title("Polynomial Basis Linear model Generator")
        ax.legend()
        ax.set_xlabel("x"); ax.set_ylabel("y")
        ax.set_ylim(-20, 20)
        plt.show()
    
    return x.flatten(), y.flatten()         

def sequential_estimator(mu, var):
    '''
    For online learning.
    Reference Website:
    https://changyaochen.github.io/welford/
    '''
    data = list()
    def __online_learning(tmp_mu, tmp_var, i ):
        global ONLINE_LEARNING_LIMIT
        nonlocal mu, var
        x = gaussian_generate(mu, var, 1)
        data.append(x)
        new_mu = tmp_mu +  (x - tmp_mu) / (i +1)
        new_var = tmp_var + ((tmp_mu - x) * (new_mu - x) - tmp_var) / (i+1)
        print(f"Add data point: {x[0]:.4f}")
        print(f"Data point source function: N({new_mu[0]:.4f}, {new_var[0]})")
        if abs(new_mu - tmp_mu) < 1e-4 and abs(new_var - tmp_var) < 1e-5 and i > 50 or \
            i> ONLINE_LEARNING_LIMIT:
            print(f"Iterate {i} times")
            return float(new_mu[0]), float(new_var[0])
        else:
            return __online_learning(new_mu, new_var, i+1)
    
    final_mean, final_var = __online_learning(0, 0, 0)
    print("Mean by algorithm: ", final_mean, "Var by algorithm: ", final_var)
    print("Mean by np: ", np.mean(data), "Var by np: ", np.var(data))
    return final_mean, final_var

def print_posterior_predictive(post_mean, post_cov, pred_mean, pred_var):
    print("\nPosterior mean:")
    for m in post_mean:
        if m[0] < 0:
            print(f" {m[0]:.8f}")
        else:
            print(f"  {m[0]:.8f}")
    print("\nPosterior variance:")
    
    for cc in post_cov:
        for c in cc:
            pad = " " if c < 0 else "  "
            print(pad + f"{c:.8f},\t", end="")
        print()
    # print(pred_var, pred_mean)
    print(f"\nPredictive distribution ~ N({pred_mean[0]:.5f}, {pred_var[0]:.5f})")

def plot_figure(n, milestone):
    # weight = np.array(weight).reshape((-1, 1))
    fig, axes = plt.subplots(2,2)
    axes = axes.flatten()
    # Precision for noise prior
    a = 1 / milestone['Ground Truth']['cov']
    
    # Plot 
    for i, (stage, values) in enumerate(milestone.items(), start=0):
        if stage.isdigit():
            axes[i].set_title(f"After {stage} outcomes")
        else:
            axes[i].set_title(stage)

        # Plot saved data during online learning.
        if stage != "Ground Truth":    
            x, y = np.array(values['x']), np.array(values['y'])
            axes[i].scatter(x, y)
        
        weight, cov = values['weight'], values['cov']
        x = np.arange(-2, 2, 0.01)
        phi = polynomial_basis(x, n, order="Nn")
        y_mean = (phi @ weight).flatten()
        y_var = cov if stage == "Ground Truth" else np.diagonal( 1/a + phi @ cov @ phi.T)

        axes[i].plot(x, y_mean, color='k')
        axes[i].plot(x, y_mean + y_var, color='r')
        axes[i].plot(x, y_mean - y_var, color='r')
        axes[i].set_ylim(-20, 20)
    plt.show()
    
def baysian_linear_regression(n, weight, noise_var, prior_precision):
    '''
    n: number of basis
    weight: Weight value for linear model.
    a: Variance for linear model data generator
    b: precision for prior
    '''
    a = 1/noise_var # Precision for y 
    weight = np.array(weight).reshape((-1, 1))
    x_list, y_list = list(), list()
    milestone = {
        'Ground Truth':{
            "x" : None,
            "y" : None,
            "weight": weight.reshape((-1, 1)),
            "cov": noise_var,
        }
    }
    w_mean, w_cov = np.zeros((n,1), dtype=np.float64), np.eye(n)/prior_precision
    record_pred_var = -1
    
    def __online_learning(tmp_w_mean, tmp_w_cov, i):
        global ONLINE_LEARNING_LIMIT
        nonlocal n, weight, a, record_pred_var
        x, y = poly_linear_model_data_generator(n, weight, noise_var, 1)
        print(f"="*70)
        print(f"Add data point ({x[0]:.4f}, {y[0]:.4f}):")
        x_list.append(x), y_list.append(y)
        phi = polynomial_basis(x, n, order='Nn')
        
        # Update Prior P(w|D)
        old_w_prec = np.linalg.inv(tmp_w_cov)
        w_prec = a * phi.T @ phi  + old_w_prec
        w_cov = np.linalg.inv(w_prec)
        w_mean = w_cov @ ( phi.T * y*a + old_w_prec @ tmp_w_mean)
        # Get coefficients of predictive distribution
        pred_mean = (phi @ w_mean).flatten()
        pred_var = (1/a + phi @w_cov @ phi.T).flatten()
        print_posterior_predictive(w_mean, w_cov, pred_mean, pred_var)
        print(f"="*70)
        
        if i == 10 or i == 50:
            milestone[f"{i}"] = {"weight":w_mean, "cov": w_cov, "x": x_list.copy(), "y":y_list.copy()}
        if  abs(pred_var- record_pred_var) < 1e-5 and i >100 or i > ONLINE_LEARNING_LIMIT:
            print(f"Iterate: {i} times")
            return w_mean, w_cov
        else:
            record_pred_var = pred_var
            return __online_learning(   tmp_w_mean=w_mean,
                                        tmp_w_cov=w_cov,
                                        i= i+1)
    
    w_mean, w_cov = __online_learning( tmp_w_mean=w_mean, 
                                        tmp_w_cov=w_cov, 
                                        i=1)
                        
    milestone['Predict Result'] = {"weight":w_mean, "cov":w_cov, "x":x_list, "y":y_list}
    plot_figure(n ,milestone)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="Machine learning homework3 for data generator.")
    parser.add_argument('task', type=str ,  help="gen_data| seq | baysian")
    args = parser.parse_args()
    
    N = 1000
    if args.task == "gen_data":
        noise_var = float(input('Input noise var (float): '))
        n = int(input("Input number of basis (Integer): "))
        tmp = input('Input weight (e.g. "1 2 3 4"): ')
        weight = [float(v.strip()) for v in tmp.split()]
        assert len(weight) == n, "Wrong number of elements for weight."
        poly_linear_model_data_generator(n, weight, noise_var, N, render=True)
    
    elif args.task == "seq":
        tmp = input('Input mean & var (e.g. "1.0 4.5"): ')
        mu, var = [float(v.strip()) for v in tmp.split(' ')]
        sequential_estimator(mu=mu, var=var)
    
    elif args.task == "baysian":
        tmp = input('Input a(noise variance) & b(prior precision) e.g. "5.5 1"): ')
        noise_var, prior_prec = [float(v.strip()) for v in tmp.split()]
        n = int(input("Input number of basis (e.g. Integer only): "))
        tmp = input('Input weight (e.g. "1 2 3 4"): ')
        weight = [float(v.strip()) for v in tmp.split()]
        
        baysian_linear_regression(n, weight, noise_var, prior_prec)
    